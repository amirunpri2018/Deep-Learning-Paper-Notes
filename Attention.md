##  hard & soft attention
1. - [ ] Xu, Kelvin, et al.  "**Show, attend and tell: Neural image caption generation with visual attention**." arxiv(2015).[pdf](arXiv preprint arXiv:1502.03044)


## global & local attention
1. - [ ] Minh-Thang Luong, Hieu Pham, Christopher D. Manning "**Effective Approaches to Attention-based Neural Machine Translation**." arxiv(2015).[pdf](https://arxiv.org/abs/1508.04025v3)

## Self Attention
1. - [ ] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, Yoshua Bengio. "**A Structured Self-attentive Sentence Embedding**." arxiv(2017).[pdf](https://arxiv.org/abs/1703.03130)

1. - [ ] Jianpeng Cheng, Li Dong and Mirella Lapata. "**Long Short-Term Memory-Networks for Machine Reading**." arxiv(2016).[pdf](https://arxiv.org/abs/1601.06733)

1. - [ ] Minh-Thang Luong, Hieu Pham, Christopher D. Manning "**Attention Is All You Need**." arxiv(2017).[pdf](https://arxiv.org/abs/1706.03762)


## Detection
1. - [x] Jianfeng Wang, Ye Yuan, Gang Yu. "**Face Attention Network: An effective Face Detector for the Occluded Faces**." CVPR(2017). [[pdf]](https://arxiv.org/abs/1711.07246)(**FAN**)

## Segmentation
1. - [ ] He, Gkioxari, et al. "**Mask R-CNN**." ICCV(2017). [[pdf]](https://arxiv.org/abs/1703.06870) (**Mask RCNN**)
